---
title: "[2] Construindo um Pipeline de Dados de uma Plataforma de E-commerce Fake até a Camada Analítica com Airflow"
subtitle: "Um projeto de engenharia de dados fim a fim"
author: "Eduardo Veloso"
date: "2024-11-09"
categories: ["Airflow", "DataEngineering", "ETL"]
image: magic_data_hub-data-flow.png
toc: true
draft: false
---

# **Introdução**

No mundo da engenharia de dados, configurar e orquestrar um pipeline de dados confiável é uma habilidade essencial. Recentemente, criei um projeto para demonstrar meu conhecimento sobre fluxos de dados usando um ambiente simulado de e-commerce. Este projeto mostra como os dados se movem da geração de pedidos até o armazenamento em um banco de dados, prontos para análise. Foi um exercício de uso de Python, Apache Airflow, PostgreSQL, Docker e S3 para construir um pipeline de ponta a ponta. Neste post, vou guiar você pelas várias etapas do projeto, as ferramentas que utilizei e alguns planos futuros para tornar o processo ainda mais robusto e escalável.

# **Visão Geral do Projeto**

O objetivo deste projeto foi simular um pipeline de dados, movendo dados de uma loja de e-commerce fake através de várias etapas de processamento, até armazená-los em um banco de dados PostgreSQL. Aqui está um resumo de cada componente do projeto:

```         
• Geração de Dados: Usando Python, criei um módulo para gerar pedidos falsos, simulando o que uma loja de e-commerce real poderia produzir.

• Ingestão e Transformação de Dados**: Esses dados foram então ingeridos, transformados e carregados (ETL) em um banco de dados PostgreSQL.

• Orquestração do Pipeline: Para gerenciar e automatizar o pipeline, usei o Apache Airflow, garantindo que cada etapa ocorresse na ordem correta e com as dependências necessárias.
```

Com essas etapas, os dados foram processados e ficaram prontos para análise, ilustrando um fluxo de trabalho típico de engenharia de dados em um cenário de e-commerce.

# **Tecnologias Utilizadas**

Aqui está uma visão geral das tecnologias que sustentaram cada parte deste pipeline:

```         
• Python: A base para a geração de dados, além de algumas das transformações.

• Apache Airflow: Orquestrou o fluxo de trabalho ETL, gerenciando dependências e agendando cada tarefa.

• S3: Usado como uma solução de armazenamento intermediário para manter dados temporariamente durante o processamento.

• Docker: Proporcionou um ambiente consistente para cada serviço, simplificando a implantação e a configuração.

• PostgreSQL: O destino final para os dados transformados, tornando-os acessíveis para análise.
```

Essa stack me permitiu criar um pipeline de dados coeso e de ponta a ponta, capaz de controlar o fluxo de dados de forma confiável.

# **Destaques do Projeto: Um Passeio pelas Etapas do Processo**

Vamos mergulhar um pouco mais em cada etapa do pipeline para entender melhor como tudo se conecta.

**1. Geração de Dados**

Usando Python, criei um módulo que gera dados sintéticos de pedidos. Esses dados incluem campos comuns de e-commerce, como ID do pedido, produto, preço e timestamp, simulando dados realistas que poderiam ser gerados por uma loja de e-commerce real.

**2. Ingestão e Transformação de Dados**

Uma vez gerados, os dados são ingeridos e passam por transformações. Alguns exemplos de transformações incluem:

```         
• Padronização de Formatos: Garantir que datas, moedas e campos de texto estejam consistentes.

• Filtragem ou Agregação: Agregar dados para simplificar a análise posterior.

• Limpeza de Dados: Remover duplicatas e lidar com valores ausentes.
```

Essas transformações preparam os dados para análise e os tornam adequados para o carregamento no banco de dados PostgreSQL.

**3. Orquestração do Pipeline com Apache Airflow**

O pipeline de dados é orquestrado com o Apache Airflow, que garante que cada etapa ocorra na ordem correta. Os DAGs (Directed Acyclic Graphs) do Airflow permitem que eu defina dependências entre tarefas, facilitando o gerenciamento de fluxos de trabalho complexos.

Principais benefícios do uso do Airflow aqui incluem:

```         
• Agendamento de Tarefas: As tarefas são agendadas e gerenciadas de forma eficiente, reduzindo a necessidade de intervenção manual.

• Tratamento de Erros: O Airflow registra e monitora as tarefas, facilitando a identificação e correção de problemas, caso ocorram.

• Escalabilidade: O Airflow é construído para escalar, o que significa que ele pode lidar com pipelines mais complexos conforme forem adicionados.
```

**4. Armazenamento no Banco de Dados**

Por fim, os dados transformados são carregados em um banco de dados PostgreSQL, que atua como a camada analítica. Armazenando os dados em um formato estruturado, eles ficam prontos para consulta e podem ser facilmente acessados para análise ou relatórios.

# **Planos Futuros: Escalando com Spark e Kubernetes**

Embora este projeto inicial tenha demonstrado com sucesso um fluxo de dados da geração ao armazenamento, já estou considerando algumas melhorias futuras:

```         
• Integração com Spark: Para lidar com conjuntos de dados maiores, pretendo usar o Apache Spark para processamento de dados em paralelo, tornando o pipeline mais robusto e capaz de lidar com big data.

• Orquestração com Kubernetes: Mover a orquestração do pipeline para o Kubernetes permitirá um melhor gerenciamento de recursos, garantindo que as tarefas sejam isoladas, resilientes e altamente disponíveis.
```

Essas melhorias não só tornarão o pipeline mais escalável, mas também o aproximarão de aplicações reais, onde as demandas de processamento de dados estão em constante crescimento.

# **Conclusão**

Este projeto foi uma experiência enriquecedora na configuração de um pipeline de dados completo. Tive a oportunidade de trabalhar com várias ferramentas poderosas, cada uma trazendo forças únicas para o processo. Com dados gerados por um módulo de e-commerce fake, orquestrados pelo Apache Airflow e armazenados no PostgreSQL, este pipeline exemplifica um fluxo de trabalho típico de engenharia de dados. No futuro, pretendo escalá-lo ainda mais com Spark e Kubernetes.

Se você estiver interessado em explorar o código, pode encontrá-lo no meu [repositório do GitHub](https://github.com/eduardoveloso/fake_ecommerce/tree/main).