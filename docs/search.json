[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Eduardo Veloso",
    "section": "",
    "text": "Meu portfólio técnico inclui proficiência em linguagens de programação e ferramentas analíticas de ponta. Sou especialista em SQL e em Python, uma linguagem versátil e poderosa para análise de dados, e em Apache Spark, a plataforma unificada para processamento de big data em larga escala.\nEm minha caixa de ferramentas, o Power BI se destaca como uma solução robusta para visualização de dados e desenvolvimento de dashboards interativos, transformando dados brutos em narrativas visuais compreensíveis. Minha experiência se estende ao uso do Data Factory e Databricks, ferramentas essenciais para a construção de pipelines de dados eficientes e análises sofisticadas na nuvem.\nPalavras-chave: Python, Apache Spark, PySpark, SQL, Power BI, Data Factory, Databricks, DA-100, MPC-70778, ETL, Power Query, Pentaho, Excel, Modelagem de Dados, Analytics, Tecnologia para Negócios."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n[2] Construindo um Pipeline de Dados de uma Plataforma de E-commerce Fake até a Camada Analítica com Airflow\n\n\nUm projeto de engenharia de dados fim a fim\n\n\n5 min\n\n\n\nNov 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[1] Data Warehouse vs Data Lake vs Lakehouse: Qual utilizar?\n\n\nEscolhendo a solução adequada\n\n\n5 min\n\n\n\nJan 15, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-01-15-repositorio-dados-big-data/index.html",
    "href": "posts/2024-01-15-repositorio-dados-big-data/index.html",
    "title": "[1] Data Warehouse vs Data Lake vs Lakehouse: Qual utilizar?",
    "section": "",
    "text": "Introdução\nCom a expansão da internet tivemos como resultado a proliferação de dados descentralizados nas redes, levando as empresas a lidar com volumes cada vez maiores de informações. Isso deu origem ao conceito de Big Data, onde a capacidade de analisar esses dados se torna valiosa para a tomada de decisões e inovação de produtos e negócios. No século XXI, os dados se tornaram a matéria-prima mais importante (quem nunca ouviu: “Os dados são o novo petróleo”) e as empresas que conseguem lidar com tamanha volumetria transformam esse mar de informações em valor para seus negócios.\n\n\n\n\n\nNo universo dos dados, profissionais se deparam constantemente com o dilema de escolher entre Data Warehouse, Data Lake e Lakehouse. Ambas as soluções têm seus méritos, mas compreendê-las profundamente é fundamental para tomar decisões informadas. Vamos mergulhar nesse tema, comparando, contrastando e destacando cenários em que uma pode ser mais vantajosa que a outra.\n\n\n\n\n\n\n\nData WareHouse\n\nA Opção Clássica para Análise Estruturada\nData Warehouses são repositórios que armazenam dados estruturados de maneira organizada e consolidada. Eles são projetados especificamente para consulta e análise, oferecendo um alto desempenho nessas operações.\nOs dados em um Data Warehouse são limpos, transformados e carregados de várias fontes, o que os torna altamente confiáveis para tomada de decisões baseadas em dados.\n\nVantagens:\nDesempenho: São altamente otimizados para operações de leitura, proporcionando respostas rápidas para consultas complexas.\nIntegridade dos Dados: A estrutura rigorosa garante a consistência e precisão dos dados, o que é crucial para a análise.\nCompatibilidade com BI: São amplamente compatíveis com ferramentas de Business Intelligence (BI), facilitando a geração de relatórios e dashboards.\n\n\nCenários de Uso:\nRelatórios e Dashboards: Para empresas que precisam de relatórios regulares e análises complexas, os Data Warehouses oferecem um ambiente estável e confiável.\nHistórico de Dados: Excelente para organizações que precisam manter um histórico detalhado de seus dados para análises retrospectivas e tendências ao longo do tempo.\n\n\n\n\n\n\n\n\n\nData Lake\n\nArmazenamento em sua Forma Mais Pura\nData Lakes são vastos repositórios que armazenam grandes volumes de dados brutos em seu formato original. A principal vantagem de um Data Lake é sua flexibilidade.\nEle pode armazenar dados estruturados, semiestruturados e não estruturados, como textos, imagens e vídeos. Essa característica os torna incrivelmente adaptáveis e ideais para big data e analytics em tempo real.\n\nVantagens:\nFlexibilidade: Não há necessidade de definir um esquema antes de armazenar os dados. Você pode guardar agora e definir a estrutura na hora de ler, o que é conhecido como schema-on-read.\nEscalabilidade: Os Data Lakes são projetados para escalar facilmente, lidando com petabytes de dados sem esforço. Custo-efetividade: Geralmente são mais baratos para manter devido à utilização de storage de baixo custo.\n\n\nCenários de Uso:\nData Science e Machine Learning: A capacidade de armazenar e processar grandes volumes de dados variados é ideal para treinar modelos de machine learning.\nAnálise de Logs e Dados de IoT: A estrutura não padronizada dos Data Lakes é perfeita para armazenar logs de servidores e dados de sensores de IoT.\n\n\n\n\n\n\n\n\n\nLakehouse\n\nO Melhor dos Dois Mundos\nO Lakehouse procura resolver as limitações dos Data Lakes e Data Warehouses, oferecendo um armazenamento de dados que une a flexibilidade dos Data Lakes com a gestão e desempenho dos Data Warehouses.\nEssa abordagem mantém os dados em um formato aberto e acessível, ao mesmo tempo em que aplica esquemas e regras de qualidade de dados, garantindo a confiabilidade necessária para análises críticas.\nO Lakehouse se tornou um novo modelo de centralizar as fontes de dados e esforços de engenharia na organização. Essencialmente, o uso do Lakehouse permite que todos os usuários possam explorar os dados, independente de suas capacidades técnicas.\nA ideia central do Lakehouse é ter um sistema de armazenamento de dados de baixo custo no data lake, utilizando um formato aberto de arquivos.\nEstes formatos de arquivo são dados estruturados com esquema de dados pré-definidos armazenados com os dados. Assim como no Data Lake, o Lakehouse separa os recursos de processamento e armazenamento, ou seja, é possível que vários motores de processamento processem os mesmos dados sem ter que armazenar os dados de forma duplicada no Data Lake e no Data Warehouse.\n\nVantagens:\nGovernança de Dados: Incorpora recursos de gestão de dados, como esquemas, controle de versões e ACID transactions, diretamente no Data Lake.\nFlexibilidade e Desempenho: Combina a capacidade de armazenar grandes volumes de dados diversos, típica dos Data Lakes, com o desempenho rápido e eficiente em consultas, característico dos Data Warehouses.\nCompatibilidade com Ferramentas de BI e ML: Facilita a integração com ferramentas de análise de dados, Business Intelligence e machine learning, oferecendo uma plataforma unificada para todas as necessidades de dados.\n\n\n\n\nConclusão\nData Lakes, Data Warehouses e Lakehouse são componentes cruciais na estratégia de dados moderna. A escolha entre essas soluções depende de uma análise cuidadosa das necessidades atuais e futuras de armazenamento, gestão e análise de dados.\nFazer a escolha da solução certa de repositório sempre dependerá de como se deseja acessar os dados, levando em consideração a velocidade e a criticidade destes dados, além de outros fatores como escalabilidade e flexibilidade de solução."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Eduardo Veloso",
    "section": "",
    "text": "Este blog é um reflexo da minha dedicação no universo de dados. Com experiencia abrangendo desde a manipulação e modelagem de dados até o desenvolvimento de soluções analíticas avançadas, tenho me dedicado a transformar grandes volumes de dados em insights acionáveis para tomada de decisões estratégicas em negócios."
  },
  {
    "objectID": "index.html#posts-recentes",
    "href": "index.html#posts-recentes",
    "title": "Eduardo Veloso",
    "section": "Posts recentes",
    "text": "Posts recentes\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[2] Construindo um Pipeline de Dados de uma Plataforma de E-commerce Fake até a Camada Analítica com Airflow\n\n\n5 min\n\n\n\nNov 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[1] Data Warehouse vs Data Lake vs Lakehouse: Qual utilizar?\n\n\n5 min\n\n\n\nJan 15, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-11-09-fakecommerce/index.html",
    "href": "posts/2024-11-09-fakecommerce/index.html",
    "title": "[2] Construindo um Pipeline de Dados de uma Plataforma de E-commerce Fake até a Camada Analítica com Airflow",
    "section": "",
    "text": "Introdução\nNo mundo da engenharia de dados, configurar e orquestrar um pipeline de dados confiável é uma habilidade essencial. Recentemente, criei um projeto para demonstrar meu conhecimento sobre fluxos de dados usando um ambiente simulado de e-commerce. Este projeto mostra como os dados se movem da geração de pedidos até o armazenamento em um banco de dados, prontos para análise. Foi um exercício de uso de Python, Apache Airflow, PostgreSQL, Docker e S3 para construir um pipeline de ponta a ponta. Neste post, vou guiar você pelas várias etapas do projeto, as ferramentas que utilizei e alguns planos futuros para tornar o processo ainda mais robusto e escalável.\n\n\nVisão Geral do Projeto\nO objetivo deste projeto foi simular um pipeline de dados, movendo dados de uma loja de e-commerce fake através de várias etapas de processamento, até armazená-los em um banco de dados PostgreSQL. Aqui está um resumo de cada componente do projeto:\n• Geração de Dados: Usando Python, criei um módulo para gerar pedidos falsos, simulando o que uma loja de e-commerce real poderia produzir.\n\n• Ingestão e Transformação de Dados**: Esses dados foram então ingeridos, transformados e carregados (ETL) em um banco de dados PostgreSQL.\n\n• Orquestração do Pipeline: Para gerenciar e automatizar o pipeline, usei o Apache Airflow, garantindo que cada etapa ocorresse na ordem correta e com as dependências necessárias.\nCom essas etapas, os dados foram processados e ficaram prontos para análise, ilustrando um fluxo de trabalho típico de engenharia de dados em um cenário de e-commerce.\n\n\nTecnologias Utilizadas\nAqui está uma visão geral das tecnologias que sustentaram cada parte deste pipeline:\n• Python: A base para a geração de dados, além de algumas das transformações.\n\n• Apache Airflow: Orquestrou o fluxo de trabalho ETL, gerenciando dependências e agendando cada tarefa.\n\n• S3: Usado como uma solução de armazenamento intermediário para manter dados temporariamente durante o processamento.\n\n• Docker: Proporcionou um ambiente consistente para cada serviço, simplificando a implantação e a configuração.\n\n• PostgreSQL: O destino final para os dados transformados, tornando-os acessíveis para análise.\nEssa stack me permitiu criar um pipeline de dados coeso e de ponta a ponta, capaz de controlar o fluxo de dados de forma confiável.\n\n\nDestaques do Projeto: Um Passeio pelas Etapas do Processo\nVamos mergulhar um pouco mais em cada etapa do pipeline para entender melhor como tudo se conecta.\n1. Geração de Dados\nUsando Python, criei um módulo que gera dados sintéticos de pedidos. Esses dados incluem campos comuns de e-commerce, como ID do pedido, produto, preço e timestamp, simulando dados realistas que poderiam ser gerados por uma loja de e-commerce real.\n2. Ingestão e Transformação de Dados\nUma vez gerados, os dados são ingeridos e passam por transformações. Alguns exemplos de transformações incluem:\n• Padronização de Formatos: Garantir que datas, moedas e campos de texto estejam consistentes.\n\n• Filtragem ou Agregação: Agregar dados para simplificar a análise posterior.\n\n• Limpeza de Dados: Remover duplicatas e lidar com valores ausentes.\nEssas transformações preparam os dados para análise e os tornam adequados para o carregamento no banco de dados PostgreSQL.\n3. Orquestração do Pipeline com Apache Airflow\nO pipeline de dados é orquestrado com o Apache Airflow, que garante que cada etapa ocorra na ordem correta. Os DAGs (Directed Acyclic Graphs) do Airflow permitem que eu defina dependências entre tarefas, facilitando o gerenciamento de fluxos de trabalho complexos.\nPrincipais benefícios do uso do Airflow aqui incluem:\n• Agendamento de Tarefas: As tarefas são agendadas e gerenciadas de forma eficiente, reduzindo a necessidade de intervenção manual.\n\n• Tratamento de Erros: O Airflow registra e monitora as tarefas, facilitando a identificação e correção de problemas, caso ocorram.\n\n• Escalabilidade: O Airflow é construído para escalar, o que significa que ele pode lidar com pipelines mais complexos conforme forem adicionados.\n4. Armazenamento no Banco de Dados\nPor fim, os dados transformados são carregados em um banco de dados PostgreSQL, que atua como a camada analítica. Armazenando os dados em um formato estruturado, eles ficam prontos para consulta e podem ser facilmente acessados para análise ou relatórios.\n\n\nPlanos Futuros: Escalando com Spark e Kubernetes\nEmbora este projeto inicial tenha demonstrado com sucesso um fluxo de dados da geração ao armazenamento, já estou considerando algumas melhorias futuras:\n• Integração com Spark: Para lidar com conjuntos de dados maiores, pretendo usar o Apache Spark para processamento de dados em paralelo, tornando o pipeline mais robusto e capaz de lidar com big data.\n\n• Orquestração com Kubernetes: Mover a orquestração do pipeline para o Kubernetes permitirá um melhor gerenciamento de recursos, garantindo que as tarefas sejam isoladas, resilientes e altamente disponíveis.\nEssas melhorias não só tornarão o pipeline mais escalável, mas também o aproximarão de aplicações reais, onde as demandas de processamento de dados estão em constante crescimento.\n\n\nConclusão\nEste projeto foi uma experiência enriquecedora na configuração de um pipeline de dados completo. Tive a oportunidade de trabalhar com várias ferramentas poderosas, cada uma trazendo forças únicas para o processo. Com dados gerados por um módulo de e-commerce fake, orquestrados pelo Apache Airflow e armazenados no PostgreSQL, este pipeline exemplifica um fluxo de trabalho típico de engenharia de dados. No futuro, pretendo escalá-lo ainda mais com Spark e Kubernetes.\nSe você estiver interessado em explorar o código, pode encontrá-lo no meu repositório do GitHub."
  }
]